{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ProteinWeaver Docs ProteinWeaver is a web interface for ontology-based protein network visualization. Background & Motivation Being able to explore how proteins are connected to other proteins with a specific function is a great tool for a biologists, as it allows them to quickly generate hypotheses that seeks to answer the ways that a protein is connected to a pathway or process. ProteinWeaver provides the tools for this type of exploration via an intuitive website that easily lets users query a protein and a specific function or process (as a gene ontology term ). Website Overview ProteinWeaver allows the users to input a protein of their interest, a specific function or process ( gene ontology term ), and the number of paths to output in the network. This generates a subnetwork that connects the protein of interest to the k shortest paths that include a protein labeled with the specific GO term. The network's information is summarised, including GO term description, links to proteins' and GO term AmiGO entry, and GO term qualifiers of the proteins. Exploration is possibly by easily interacting with the graph and setting new nodes as the protein of interest. Queries are easily reproduced through exporting a log history of all queries and explorations done in a session, and exporting networks via images.","title":"Home"},{"location":"#welcome-to-proteinweaver-docs","text":"ProteinWeaver is a web interface for ontology-based protein network visualization.","title":"Welcome to ProteinWeaver Docs"},{"location":"#background-motivation","text":"Being able to explore how proteins are connected to other proteins with a specific function is a great tool for a biologists, as it allows them to quickly generate hypotheses that seeks to answer the ways that a protein is connected to a pathway or process. ProteinWeaver provides the tools for this type of exploration via an intuitive website that easily lets users query a protein and a specific function or process (as a gene ontology term ).","title":"Background &amp; Motivation"},{"location":"#website-overview","text":"ProteinWeaver allows the users to input a protein of their interest, a specific function or process ( gene ontology term ), and the number of paths to output in the network. This generates a subnetwork that connects the protein of interest to the k shortest paths that include a protein labeled with the specific GO term. The network's information is summarised, including GO term description, links to proteins' and GO term AmiGO entry, and GO term qualifiers of the proteins. Exploration is possibly by easily interacting with the graph and setting new nodes as the protein of interest. Queries are easily reproduced through exporting a log history of all queries and explorations done in a session, and exporting networks via images.","title":"Website Overview"},{"location":"data-import/","text":"Data Import Instructions How to get started with Neo4j and upload the data Create a directory in your $HOME named neo4j Within ~/neo4j directory create the following directories: ~/neo4j/data/ to allow storage of data between docker instances ~/neo4j/logs/ to allow storage of logs between docker instances ~/neo4j/import/ to import data Load any FlyBase data by copying interactome-flybase-collapsed-weighted.txt into import directory Delete 'sy#' preceding the first column name in interactome-flybase-collapsed-weighted.txt Import the properly formatted GO terms file from FlyBase and store in the GitHub repository: gene_association.fb . ~/neo4j/plugins/ to store any necessary plugins for production environments Create a docker instance with APOC plugin using the following command: docker run \\ --name proteinweaver \\ -p7474:7474 -p7687:7687 \\ -v $HOME/neo4j/data:/data \\ -v $HOME/neo4j/logs:/logs \\ -v $HOME/neo4j/import:/import \\ -v $HOME/neo4j/plugins:/plugins \\ --env NEO4J_AUTH=none \\ -e NEO4J_apoc_export_file_enabled=true \\ -e NEO4J_apoc_import_file_enabled=true \\ -e NEO4J_apoc_import_file_use__neo4j__config=true \\ -e NEO4J_PLUGINS='[\"graph-data-science\"]' \\ -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\ neo4j:latest This docker instance has no security restrictions, to change username and password edit: --env NEO4J_AUTH=username/password Access the docker image at http://localhost:7474 Create constraints before data import. We use NCBI as the source of the unique taxon identifiers. CREATE CONSTRAINT txid_constraint FOR (n:protein) REQUIRE (n.txid, n.id) IS UNIQUE; Create a constraint for the GO terms in the database using the following command: CREATE CONSTRAINT go_constraint FOR (n:go_term) REQUIRE n.id IS UNIQUE; Import D. melanogaster protein interactome using the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///interactome-flybase-collapsed-weighted.txt' AS fly FIELDTERMINATOR '\\t' CALL { with fly MERGE (a:protein {id: fly.FlyBase1, name: fly.symbol1, txid: \"txid7227\", species: \"Drosophila melanogaster\"}) MERGE (b:protein {id: fly.FlyBase2, name: fly.symbol2, txid: \"txid7227\", species: \"Drosophila melanogaster\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; This will create all of the protein-protein relationships and populate the database. Set a relationship property for the PubmedID :auto LOAD CSV WITH HEADERS FROM 'file:///interactome-flybase-collapsed-weighted.txt' AS fly FIELDTERMINATOR '\\t' CALL { with fly MATCH (s:protein {id: fly.FlyBase1, txid:\"txid7227\"})-[r:ProPro]-(t:protein {id: fly.FlyBase2, txid: \"txid7227\"}) SET r.pubmed_id = fly.PubMedIDs } IN TRANSACTIONS OF 1000 ROWS; Import the Gene Ontology data, gene_association.fb , into the database using the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///gene_association.fb' AS flygo FIELDTERMINATOR '\\t' CALL { with flygo MATCH (n:protein {id: flygo.db_object_id, txid:\"txid7227\"}) MERGE (g:go_term {id: flygo.go_id}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Import the relationships qualifiers for the GO terms and fly proteins using the following commands: :auto LOAD CSV WITH HEADERS FROM 'file:///gene_association.fb' AS flygo FIELDTERMINATOR '\\t' CALL { with flygo MATCH (p:protein {id: flygo.db_object_id, txid:\"txid7227\"})-[r:ProGo]-(g:go_term {id: flygo.go_id}) SET r.relationship = flygo.qualifier } IN TRANSACTIONS OF 1000 ROWS; Import B. subtilis protein interactome with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_interactome.csv' AS bsub CALL { with bsub MERGE (a:protein {id: bsub.protein_1_locus, name: bsub.protein_1_name, txid: \"txid224308\", species: \"Bacillus subtilis 168\"}) MERGE (b:protein {id: bsub.protein_2_locus, name: bsub.protein_2_name, txid: \"txid224308\", species: \"Bacillus subtilis 168\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; Add GO data to B. subtilis nodes: :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data.csv' AS bsubgo CALL { with bsubgo MATCH (n:protein {id: bsubgo.locus, txid: \"txid224308\"}) MERGE (g:go_term {id: bsubgo.go_term}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for B. subtilis . :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data.csv' AS bsubgo CALL { with bsubgo MATCH (p:protein {id: bsubgo.locus, txid: \"txid224308\"})-[r:ProGo]-(g:go_term {id: bsubgo.go_term}) SET r.relationship = bsubgo.qualifier } IN TRANSACTIONS OF 1000 ROWS; Download and import the GO hierarchy using the commands below. This is made from the script ParseOntologyRelationship.ipynb if you are interested. :auto LOAD CSV WITH HEADERS FROM 'file:///is_a_import.tsv' AS go FIELDTERMINATOR '\\t' CALL { with go MERGE (a:go_term {id: go.id}) MERGE (b:go_term {id: go.id2}) MERGE (a)-[r:GoGo]->(b) SET r.relationship = go.is_a } IN TRANSACTIONS OF 100 ROWS; Download and import the GO term common names and descriptions with the Cypher commands below. This file is made from the script ParseOBOtoTXT.ipynb if you are interested. :auto LOAD CSV WITH HEADERS FROM 'file:///go.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.name = go.name, n.namespace = go.namespace, n.def = go.def } IN TRANSACTIONS OF 1000 ROWS; March 18, 2024 Major Data Update: Don't forget to drop the existing projection before adding more data. call gds.graph.drop(\"proGoGraph\") YIELD graphName Import more GO data for D. melanogaster :auto LOAD CSV WITH HEADERS FROM 'file:///dmel_GO_data_Mar15_24.tsv' AS dmelgo FIELDTERMINATOR '\\t' CALL { with dmelgo MATCH (n:protein {id: dmelgo.FB_ID, txid: \"txid7227\"}) MERGE (g:go_term {id: dmelgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for D. melanogaster . :auto LOAD CSV WITH HEADERS FROM 'file:///dmel_GO_data_Mar15_24.tsv' AS dmelgo FIELDTERMINATOR '\\t' CALL { with dmelgo MATCH (p:protein {id: dmelgo.FB_ID, txid: \"txid7227\"})-[r:ProGo]-(g:go_term {id: dmelgo.GO_TERM}) SET r.relationship = dmelgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import more GO data for B. subtilis :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data_Mar18_24.tsv' AS bsubgo FIELDTERMINATOR '\\t' CALL { with bsubgo MATCH (n:protein {id: bsubgo.BSU_ID, txid: \"txid224308\"}) MERGE (g:go_term {id: bsubgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for B. subtilis . :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data_Mar18_24.tsv' AS bsubgo FIELDTERMINATOR '\\t' CALL { with bsubgo MATCH (p:protein {id: bsubgo.BSU_ID, txid: \"txid224308\"})-[r:ProGo]-(g:go_term {id: bsubgo.GO_TERM}) SET r.relationship = bsubgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import D. rerio protein interactome with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_interactome_Mar12_2024.txt' AS zfish FIELDTERMINATOR '\\t' CALL { with zfish MERGE (a:protein {id: zfish.uniprotID1, name: zfish.name1, txid: \"txid7955\", species: \"Danio rerio\"}) MERGE (b:protein {id: zfish.uniprotID2, name: zfish.name2, txid: \"txid7955\", species: \"Danio rerio\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; Set a relationship property for the evidence :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_interactome_Mar12_2024.txt' AS zfish FIELDTERMINATOR '\\t' CALL { with zfish MATCH (s:protein {id: zfish.uniprotID1, txid: \"txid7955\"})-[r:ProPro]-(t:protein {id: zfish.uniprotID2, txid: \"txid7955\"}) SET r.evidence = zfish.evidence } IN TRANSACTIONS OF 1000 ROWS; Add GO data to D. rerio nodes: :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_GO_data_Mar12_24.tsv' AS zfishgo FIELDTERMINATOR '\\t' CALL { with zfishgo MATCH (n:protein {id: zfishgo.GENE_PRODUCT_ID, txid: \"txid7955\"}) MERGE (g:go_term {id: zfishgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for D. rerio . :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_GO_data_Mar12_24.tsv' AS zfishgo FIELDTERMINATOR '\\t' CALL { with zfishgo MATCH (p:protein {id: zfishgo.GENE_PRODUCT_ID, txid: \"txid7955\"})-[r:ProGo]-(g:go_term {id: zfishgo.GO_TERM}) SET r.relationship = zfishgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import the GO hierarchy with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///is_a_import.tsv' AS go FIELDTERMINATOR '\\t' CALL { with go MERGE (a:go_term {id: go.id}) MERGE (b:go_term {id: go.id2}) MERGE (a)-[r:GoGo]->(b) SET r.relationship = go.is_a } IN TRANSACTIONS OF 100 ROWS; Import the GO term common names and descriptions with the following Cypher command: :auto LOAD CSV WITH HEADERS FROM 'file:///go.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.name = go.name, n.namespace = go.namespace, n.def = go.def } IN TRANSACTIONS OF 1000 ROWS; Mar. 28, 2024 Add blacklist indicator to GO term nodes from new dataset : :auto LOAD CSV WITH HEADERS FROM 'file:///go_2024-03-28.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.never_annotate = go.never_annotate } IN TRANSACTIONS OF 1000 ROWS; April. 1, 2024 We added inferred ProGo edges from descendant ProGo edges. This means that proteins annotated to a specific GO term, such as Mbs to enzyme inhibitor activity, will also be annotated to that GO term's ancestors, such as molecular function inhibitor activity and molecular_function. We need to create more ProGo edges across all the species using the following commands: Add ancestral edges for D. rerio . MATCH (p:protein {txid: 'txid7955'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add ancestral edges for B. subtilis . MATCH (p:protein {txid: 'txid224308'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add ancestral edges for D. melanogaster . MATCH (p:protein {txid: 'txid7227'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add qualifiers for new ProGo edges. MATCH (p:protein)-[r:ProGo]-(g:go_term) WHERE r.relationship IS NULL SET r.relationship = \"inferred_from_descendant\" Species Relationships Added D. Melanogaster 415,493 B. Subtilis 39,215 D.Rerio 86,304 Total 541,012 Call the graph projection CALL gds.graph.project( 'proGoGraph', { go_term: { label: 'go_term' }, protein: { label: 'protein' } }, { ProGo: { type: 'ProGo', orientation: 'NATURAL', properties: {} }, ProPro: { type: 'ProPro', orientation: 'UNDIRECTED', properties: {} } } ); Useful Commands Delete nodes: MATCH (n:protein {txid: \"example\", species: \"example\"}) DETACH DELETE n Drop constraints: DROP CONSTRAINT constraint Drop graph projection: CALL gds.graph.drop('proGoGraph') YIELD graphName Show database information: :schema","title":"Data Import"},{"location":"data-import/#data-import-instructions","text":"","title":"Data Import Instructions"},{"location":"data-import/#how-to-get-started-with-neo4j-and-upload-the-data","text":"Create a directory in your $HOME named neo4j Within ~/neo4j directory create the following directories: ~/neo4j/data/ to allow storage of data between docker instances ~/neo4j/logs/ to allow storage of logs between docker instances ~/neo4j/import/ to import data Load any FlyBase data by copying interactome-flybase-collapsed-weighted.txt into import directory Delete 'sy#' preceding the first column name in interactome-flybase-collapsed-weighted.txt Import the properly formatted GO terms file from FlyBase and store in the GitHub repository: gene_association.fb . ~/neo4j/plugins/ to store any necessary plugins for production environments Create a docker instance with APOC plugin using the following command: docker run \\ --name proteinweaver \\ -p7474:7474 -p7687:7687 \\ -v $HOME/neo4j/data:/data \\ -v $HOME/neo4j/logs:/logs \\ -v $HOME/neo4j/import:/import \\ -v $HOME/neo4j/plugins:/plugins \\ --env NEO4J_AUTH=none \\ -e NEO4J_apoc_export_file_enabled=true \\ -e NEO4J_apoc_import_file_enabled=true \\ -e NEO4J_apoc_import_file_use__neo4j__config=true \\ -e NEO4J_PLUGINS='[\"graph-data-science\"]' \\ -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\ neo4j:latest This docker instance has no security restrictions, to change username and password edit: --env NEO4J_AUTH=username/password Access the docker image at http://localhost:7474 Create constraints before data import. We use NCBI as the source of the unique taxon identifiers. CREATE CONSTRAINT txid_constraint FOR (n:protein) REQUIRE (n.txid, n.id) IS UNIQUE; Create a constraint for the GO terms in the database using the following command: CREATE CONSTRAINT go_constraint FOR (n:go_term) REQUIRE n.id IS UNIQUE; Import D. melanogaster protein interactome using the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///interactome-flybase-collapsed-weighted.txt' AS fly FIELDTERMINATOR '\\t' CALL { with fly MERGE (a:protein {id: fly.FlyBase1, name: fly.symbol1, txid: \"txid7227\", species: \"Drosophila melanogaster\"}) MERGE (b:protein {id: fly.FlyBase2, name: fly.symbol2, txid: \"txid7227\", species: \"Drosophila melanogaster\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; This will create all of the protein-protein relationships and populate the database. Set a relationship property for the PubmedID :auto LOAD CSV WITH HEADERS FROM 'file:///interactome-flybase-collapsed-weighted.txt' AS fly FIELDTERMINATOR '\\t' CALL { with fly MATCH (s:protein {id: fly.FlyBase1, txid:\"txid7227\"})-[r:ProPro]-(t:protein {id: fly.FlyBase2, txid: \"txid7227\"}) SET r.pubmed_id = fly.PubMedIDs } IN TRANSACTIONS OF 1000 ROWS; Import the Gene Ontology data, gene_association.fb , into the database using the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///gene_association.fb' AS flygo FIELDTERMINATOR '\\t' CALL { with flygo MATCH (n:protein {id: flygo.db_object_id, txid:\"txid7227\"}) MERGE (g:go_term {id: flygo.go_id}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Import the relationships qualifiers for the GO terms and fly proteins using the following commands: :auto LOAD CSV WITH HEADERS FROM 'file:///gene_association.fb' AS flygo FIELDTERMINATOR '\\t' CALL { with flygo MATCH (p:protein {id: flygo.db_object_id, txid:\"txid7227\"})-[r:ProGo]-(g:go_term {id: flygo.go_id}) SET r.relationship = flygo.qualifier } IN TRANSACTIONS OF 1000 ROWS; Import B. subtilis protein interactome with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_interactome.csv' AS bsub CALL { with bsub MERGE (a:protein {id: bsub.protein_1_locus, name: bsub.protein_1_name, txid: \"txid224308\", species: \"Bacillus subtilis 168\"}) MERGE (b:protein {id: bsub.protein_2_locus, name: bsub.protein_2_name, txid: \"txid224308\", species: \"Bacillus subtilis 168\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; Add GO data to B. subtilis nodes: :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data.csv' AS bsubgo CALL { with bsubgo MATCH (n:protein {id: bsubgo.locus, txid: \"txid224308\"}) MERGE (g:go_term {id: bsubgo.go_term}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for B. subtilis . :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data.csv' AS bsubgo CALL { with bsubgo MATCH (p:protein {id: bsubgo.locus, txid: \"txid224308\"})-[r:ProGo]-(g:go_term {id: bsubgo.go_term}) SET r.relationship = bsubgo.qualifier } IN TRANSACTIONS OF 1000 ROWS; Download and import the GO hierarchy using the commands below. This is made from the script ParseOntologyRelationship.ipynb if you are interested. :auto LOAD CSV WITH HEADERS FROM 'file:///is_a_import.tsv' AS go FIELDTERMINATOR '\\t' CALL { with go MERGE (a:go_term {id: go.id}) MERGE (b:go_term {id: go.id2}) MERGE (a)-[r:GoGo]->(b) SET r.relationship = go.is_a } IN TRANSACTIONS OF 100 ROWS; Download and import the GO term common names and descriptions with the Cypher commands below. This file is made from the script ParseOBOtoTXT.ipynb if you are interested. :auto LOAD CSV WITH HEADERS FROM 'file:///go.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.name = go.name, n.namespace = go.namespace, n.def = go.def } IN TRANSACTIONS OF 1000 ROWS;","title":"How to get started with Neo4j and upload the data"},{"location":"data-import/#march-18-2024-major-data-update","text":"Don't forget to drop the existing projection before adding more data. call gds.graph.drop(\"proGoGraph\") YIELD graphName Import more GO data for D. melanogaster :auto LOAD CSV WITH HEADERS FROM 'file:///dmel_GO_data_Mar15_24.tsv' AS dmelgo FIELDTERMINATOR '\\t' CALL { with dmelgo MATCH (n:protein {id: dmelgo.FB_ID, txid: \"txid7227\"}) MERGE (g:go_term {id: dmelgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for D. melanogaster . :auto LOAD CSV WITH HEADERS FROM 'file:///dmel_GO_data_Mar15_24.tsv' AS dmelgo FIELDTERMINATOR '\\t' CALL { with dmelgo MATCH (p:protein {id: dmelgo.FB_ID, txid: \"txid7227\"})-[r:ProGo]-(g:go_term {id: dmelgo.GO_TERM}) SET r.relationship = dmelgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import more GO data for B. subtilis :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data_Mar18_24.tsv' AS bsubgo FIELDTERMINATOR '\\t' CALL { with bsubgo MATCH (n:protein {id: bsubgo.BSU_ID, txid: \"txid224308\"}) MERGE (g:go_term {id: bsubgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for B. subtilis . :auto LOAD CSV WITH HEADERS FROM 'file:///bsub_GO_data_Mar18_24.tsv' AS bsubgo FIELDTERMINATOR '\\t' CALL { with bsubgo MATCH (p:protein {id: bsubgo.BSU_ID, txid: \"txid224308\"})-[r:ProGo]-(g:go_term {id: bsubgo.GO_TERM}) SET r.relationship = bsubgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import D. rerio protein interactome with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_interactome_Mar12_2024.txt' AS zfish FIELDTERMINATOR '\\t' CALL { with zfish MERGE (a:protein {id: zfish.uniprotID1, name: zfish.name1, txid: \"txid7955\", species: \"Danio rerio\"}) MERGE (b:protein {id: zfish.uniprotID2, name: zfish.name2, txid: \"txid7955\", species: \"Danio rerio\"}) MERGE (a)-[r:ProPro]-(b) } IN TRANSACTIONS OF 100 ROWS; Set a relationship property for the evidence :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_interactome_Mar12_2024.txt' AS zfish FIELDTERMINATOR '\\t' CALL { with zfish MATCH (s:protein {id: zfish.uniprotID1, txid: \"txid7955\"})-[r:ProPro]-(t:protein {id: zfish.uniprotID2, txid: \"txid7955\"}) SET r.evidence = zfish.evidence } IN TRANSACTIONS OF 1000 ROWS; Add GO data to D. rerio nodes: :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_GO_data_Mar12_24.tsv' AS zfishgo FIELDTERMINATOR '\\t' CALL { with zfishgo MATCH (n:protein {id: zfishgo.GENE_PRODUCT_ID, txid: \"txid7955\"}) MERGE (g:go_term {id: zfishgo.GO_TERM}) MERGE (n)-[r:ProGo]-(g) } IN TRANSACTIONS OF 1000 ROWS; Set qualifier property for D. rerio . :auto LOAD CSV WITH HEADERS FROM 'file:///zfish_GO_data_Mar12_24.tsv' AS zfishgo FIELDTERMINATOR '\\t' CALL { with zfishgo MATCH (p:protein {id: zfishgo.GENE_PRODUCT_ID, txid: \"txid7955\"})-[r:ProGo]-(g:go_term {id: zfishgo.GO_TERM}) SET r.relationship = zfishgo.QUALIFIER } IN TRANSACTIONS OF 1000 ROWS; Import the GO hierarchy with the following command: :auto LOAD CSV WITH HEADERS FROM 'file:///is_a_import.tsv' AS go FIELDTERMINATOR '\\t' CALL { with go MERGE (a:go_term {id: go.id}) MERGE (b:go_term {id: go.id2}) MERGE (a)-[r:GoGo]->(b) SET r.relationship = go.is_a } IN TRANSACTIONS OF 100 ROWS; Import the GO term common names and descriptions with the following Cypher command: :auto LOAD CSV WITH HEADERS FROM 'file:///go.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.name = go.name, n.namespace = go.namespace, n.def = go.def } IN TRANSACTIONS OF 1000 ROWS;","title":"March 18, 2024 Major Data Update:"},{"location":"data-import/#mar-28-2024","text":"Add blacklist indicator to GO term nodes from new dataset : :auto LOAD CSV WITH HEADERS FROM 'file:///go_2024-03-28.txt' AS go FIELDTERMINATOR '\\t' CALL { with go MATCH (n:go_term {id: go.id}) SET n.never_annotate = go.never_annotate } IN TRANSACTIONS OF 1000 ROWS;","title":"Mar. 28, 2024"},{"location":"data-import/#april-1-2024","text":"We added inferred ProGo edges from descendant ProGo edges. This means that proteins annotated to a specific GO term, such as Mbs to enzyme inhibitor activity, will also be annotated to that GO term's ancestors, such as molecular function inhibitor activity and molecular_function. We need to create more ProGo edges across all the species using the following commands: Add ancestral edges for D. rerio . MATCH (p:protein {txid: 'txid7955'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add ancestral edges for B. subtilis . MATCH (p:protein {txid: 'txid224308'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add ancestral edges for D. melanogaster . MATCH (p:protein {txid: 'txid7227'})-[:ProGo]-(g:go_term) WITH p, collect(g) AS go_terms UNWIND go_terms as go_input MATCH (p)-[:ProGo]-(g:go_term {id: go_input.id})-[:GoGo*]->(g2) WITH p, collect(distinct g2) AS parent_terms UNWIND parent_terms AS parent_term MERGE (p)-[r:ProGo]-(parent_term) Add qualifiers for new ProGo edges. MATCH (p:protein)-[r:ProGo]-(g:go_term) WHERE r.relationship IS NULL SET r.relationship = \"inferred_from_descendant\" Species Relationships Added D. Melanogaster 415,493 B. Subtilis 39,215 D.Rerio 86,304 Total 541,012","title":"April. 1, 2024"},{"location":"data-import/#call-the-graph-projection","text":"CALL gds.graph.project( 'proGoGraph', { go_term: { label: 'go_term' }, protein: { label: 'protein' } }, { ProGo: { type: 'ProGo', orientation: 'NATURAL', properties: {} }, ProPro: { type: 'ProPro', orientation: 'UNDIRECTED', properties: {} } } );","title":"Call the graph projection"},{"location":"data-import/#useful-commands","text":"Delete nodes: MATCH (n:protein {txid: \"example\", species: \"example\"}) DETACH DELETE n Drop constraints: DROP CONSTRAINT constraint Drop graph projection: CALL gds.graph.drop('proGoGraph') YIELD graphName Show database information: :schema","title":"Useful Commands"},{"location":"setup/","text":"Setup The setup guide will include the backend database, server, and the frontend Backend Database Neo4j is the database. We will need Docker to setup the database. Follow this page to install the Docker Desktop. First pull the official Neo4j Docker image docker pull neo4j Run the Neo4j Docker image docker run --interactive --tty -p7687:7687 -p7474:7474 -p7473:7473 --env NEO4J_AUTH=neo4j/testpassword --env NEO4J_ACCEPT_LICENSE_AGREEMENT=yes neo4j:5.11.0-community-bullseye Visit your http://localhost:7474/browser/ to view the database. You will need to input the username and password credentials which is outlined in the previous docker command. For the first iteration, we need to manually input the Movies database provided by Neo4j. You can create this data by opening the live data guide, and inputting the first code block in the editor. The database is now hosted locally, and you will need to keep the terminal window open Backend Server The backend server is run using Express.js Open a new terminal window and clone the repository. Locate the server in the server directory cd server Next we need to install node.js, and the recommended way is to use an Node Version Manager. Follow the NVM Github instruction before proceeding. The node version is outlined in the .nvmrc file in both of the client and server directories. Follow the command below to use the correct version. nvm use If the node version is not installed, follow the instuction to do so. npm install You can verify the node version currently being used using: node -v To start the server, do the following function: npm start The server should be running on http://localhost:3000/ . There are several APIs, and you can verify it works by using http://localhost:3000/api/test which should output a JSON object. Please keep the terminal window open. Frontend The frontend uses the React.js framework, and uses Vite.js as a bundler. Open a new terminal window and clone the repository. Locate the server in the server directory cd client Similar to the backend server setup, we need to use and install the correct node.js version. Follow the commands below, and you may need to install the correct node version: nvm use npm install node -v To start the frontend, do the following command: npm run dev The website should be running on http://localhost:5173/","title":"Setup"},{"location":"setup/#setup","text":"The setup guide will include the backend database, server, and the frontend","title":"Setup"},{"location":"setup/#backend-database","text":"Neo4j is the database. We will need Docker to setup the database. Follow this page to install the Docker Desktop. First pull the official Neo4j Docker image docker pull neo4j Run the Neo4j Docker image docker run --interactive --tty -p7687:7687 -p7474:7474 -p7473:7473 --env NEO4J_AUTH=neo4j/testpassword --env NEO4J_ACCEPT_LICENSE_AGREEMENT=yes neo4j:5.11.0-community-bullseye Visit your http://localhost:7474/browser/ to view the database. You will need to input the username and password credentials which is outlined in the previous docker command. For the first iteration, we need to manually input the Movies database provided by Neo4j. You can create this data by opening the live data guide, and inputting the first code block in the editor. The database is now hosted locally, and you will need to keep the terminal window open","title":"Backend Database"},{"location":"setup/#backend-server","text":"The backend server is run using Express.js Open a new terminal window and clone the repository. Locate the server in the server directory cd server Next we need to install node.js, and the recommended way is to use an Node Version Manager. Follow the NVM Github instruction before proceeding. The node version is outlined in the .nvmrc file in both of the client and server directories. Follow the command below to use the correct version. nvm use If the node version is not installed, follow the instuction to do so. npm install You can verify the node version currently being used using: node -v To start the server, do the following function: npm start The server should be running on http://localhost:3000/ . There are several APIs, and you can verify it works by using http://localhost:3000/api/test which should output a JSON object. Please keep the terminal window open.","title":"Backend Server"},{"location":"setup/#frontend","text":"The frontend uses the React.js framework, and uses Vite.js as a bundler. Open a new terminal window and clone the repository. Locate the server in the server directory cd client Similar to the backend server setup, we need to use and install the correct node.js version. Follow the commands below, and you may need to install the correct node version: nvm use npm install node -v To start the frontend, do the following command: npm run dev The website should be running on http://localhost:5173/","title":"Frontend"}]}